{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5da02b9",
   "metadata": {},
   "source": [
    "# Optimality conditions\n",
    "\n",
    "- ## $f$ not necessarily convex.\n",
    "- ## for unrestricted problems\n",
    "\n",
    "$$min~f(x) $$\n",
    "$$ X \\in \\mathbb{R}^n $$\n",
    "\n",
    "or which is the same\n",
    "\n",
    "$$min~f(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948fe1ea",
   "metadata": {},
   "source": [
    "## Proposition (Condicion necesariza de optimalidad)\n",
    "\n",
    "Sea $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$ diferenciable y $x^*$ un mínimo local.\n",
    "\n",
    "$\\Rightarrow~~~ \\nabla f(x^*)=0~~~~~$ \n",
    "\n",
    "Si además, $f \\in C^2 ~~~~\\Rightarrow~~~ \\nabla^2 f(x^*)$ es semidefinida positiva.\n",
    "\n",
    "### Demonstration\n",
    "\n",
    "Sea $v \\in \\mathbb{R}^n$\n",
    "\n",
    "$ 0 \\leq \\lim_{h \\rightarrow 0^+}f(x^*+hv) - f(x^*) = \\nabla f(x^*)^T v\\geq 0$\n",
    "\n",
    "Lo mismo vale para $-v$\n",
    "\n",
    "$\\nabla f(x^*)^T(-v) \\geq 0 ~~~~~ \\Rightarrow ~~~~~ \\nabla f(x^*)^T(v) \\leq 0$\n",
    "\n",
    "Tenemos:\n",
    "\n",
    "$$\\left.\\begin{matrix}\n",
    "< nabla f(x^*)^T,v >~~~ \\geq 0   \\\\\n",
    "< nabla f(x^*)^T,v > ~~~ \\leq 0  \n",
    "\\end{matrix}\\right\\}  \\Rightarrow < nabla f(x^*)^T,v > = 0  $$\n",
    "\n",
    "\n",
    "Además, esto es cierto $\\forall~v~~~~~ \\Rightarrow~~~~~ \\nabla f(x^*)= 0 ~~~ \\blacksquare $\n",
    "\n",
    "\n",
    "Si además $f \\in C^2$:\n",
    "\n",
    "Hacemos el desarrollo de Tylor:\n",
    "\n",
    "$f(x^*+hv) = f(x^*) + \\dfrac{h^2}{2}v^T\\nabla^2 f(x^*)v + o(h^2)$ \n",
    "\n",
    "Donde ya no tomamos en cuenta la primer diferenciacion que es 0.\n",
    "\n",
    "Reordenando:\n",
    "\n",
    "$f(x^*+hv) - f(x^*) = \\dfrac{h^2}{2}v^T\\nabla^2 f(x^*)v + o(h^2) \\geq 0$\n",
    "\n",
    "Esto es mayor a cero por la mismo que mostramos en el paso anterior, además puedo dividir entre $h^2$\n",
    "\n",
    "$\\dfrac{f(x^*+hv) - f(x^*)}{h^2} =  v^T\\nabla^2 f(x^*)v + \\dfrac{o(h^2)}{h^2} \\geq 0$\n",
    "\n",
    "como $ \\lim_{h\\rightarrow 0} \\dfrac{o(h^2)}{h^2} \\rightarrow 0~~~~\\Rightarrow~~~~~ v^T\\nabla^2 f(x^*)v \\geq 0 ~~~\\blacksquare $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77b950",
   "metadata": {},
   "source": [
    "## Proposition\n",
    "\n",
    "Sea $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ diferenciable y convexa\n",
    "\n",
    "$\\Rightarrow~~~ \\nabla f(x^*) = 0$ es condicion necesaria y suficiente para que $x^*$ sea mínimo global\n",
    "\n",
    "\n",
    "### Demonstration\n",
    "\n",
    "We know that $f(x)\\geq f(x^*) + \\nabla f(x^*)^T (x-x^*)~~~~~ \\forall x$\n",
    "\n",
    "If $\\nabla f(x^*)=0~~~ \\Rightarrow ~~~ f(x) \\geq f(x^*)~~~~~ \\forall x ~~~\\blacksquare $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a89a37",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfc83f",
   "metadata": {},
   "source": [
    "# Optimization algorithms\n",
    "\n",
    "- ## Almost never exist closed solutions\n",
    "- ## So we consider iterative methods\n",
    "    - ### Bisection method (\"Golden section search\")\n",
    "    - ### descent algorithms (\"Gradient\",  \"Newton\")\n",
    "    - ### Proximal/ fixed point\n",
    "    - ### Inner point\n",
    "    - ### Stochastic GD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d28a6",
   "metadata": {},
   "source": [
    "## Descent/Gradient Methods\n",
    "\n",
    "### Considering unrestricted problem.\n",
    "\n",
    "Let start from a point $x^0$ and generate $x^1,~x^2,~x^3.... $ a sequence.\n",
    "\n",
    "- If $f(x^{k+1})<f(x^{k})~~~ \\forall~k~~~~~\\Rightarrow$ we say that it is a descent method\n",
    "\n",
    "Let's take:  \n",
    "\n",
    "$x^{k+1}= x^k + \\alpha d^k$  \n",
    "\n",
    "where:\n",
    "- $d^k \\in \\mathbb{R}^n$ is a direction\n",
    "- $\\alpha^k \\in \\mathbb{R}^+$ is the size of the step\n",
    "\n",
    "<img src='Des_Esq.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc47749",
   "metadata": {},
   "source": [
    "## How to select $d^k$  y  $\\alpha^k$ ??\n",
    "\n",
    "### Working with differenciable funtions\n",
    "\n",
    "An example could be:\n",
    "\n",
    "$d^k = - \\nabla f(x^k)$ (Here appear the gradient...)\n",
    "\n",
    "<img src='direction.png'>\n",
    "\n",
    "More general:\n",
    "\n",
    "Let's search a $d^k /~~ <\\nabla f(x^*), d^k>~<~0$ (*) $~~~ \\leftarrow ~~~ $Known as gradient methods\n",
    "\n",
    "\n",
    "- There are methods of descent that aren't gradient methods\n",
    "- Not all gradient method is a descent method\n",
    "\n",
    "<img src='Prod_int_neg.png'>\n",
    "\n",
    "This condition (*) guarantee that $f$ locally decrease in that direction.\n",
    "\n",
    "In other words: $\\exists \\delta >0 /~~~ f(x^k+\\alpha^k d^k) < f(x^k)~~~~~ if~\\alpha^k \\in (0,\\delta)$\n",
    "\n",
    "(*) means $\\dfrac{\\partial f}{\\partial d^k} < 0$ The directional derivative is negative.\n",
    "\n",
    "### Then, this method is:\n",
    "\n",
    "Repeat\n",
    "- Chose direction $d^k$\n",
    "- Chose step $alpha^k$\n",
    "- Do $x^{k+1} = x^k+ \\alpha\"k d^k$\n",
    "\n",
    "Untill stop condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6de1f5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
