{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6c4a61",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "$$x^{k+1} = x^k + \\alpha^kd^k$$\n",
    "\n",
    "$$<~\\nabla f(x^k), d^k~~>~~<~~0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892bb0d",
   "metadata": {},
   "source": [
    "## How to Choose the direction\n",
    "\n",
    "Re-write the basic equation:\n",
    "\n",
    "$x^{k+1} = x^k - \\alpha^k D^k \\nabla f(x^k)~~~~~~~~~~$       with  $~~ D^k~\\in~S^n_{++}\\leftarrow$ Positive Defined and symetric\n",
    "\n",
    "...and the condition:\n",
    "\n",
    "$\\nabla f(x^k)D^k\\nabla f(x^k)~~>~~ 0~~\\leftarrow~~ D^k$ is Positive Defined and symetric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf0b2f",
   "metadata": {},
   "source": [
    "### Options for $D^k$\n",
    "\n",
    "1. $D = I~~~~~\\rightarrow d^k = -\\nabla f(x^k)~~~~~\\rightarrow~~~~~ x^{k+1} = x^k - \\alpha^k  \\nabla f(x^k) $\n",
    "\n",
    "    This is not always the best option because sometimes it makes you zigzag in your path.\n",
    "    This depends on the form of the level curves, but is cheap.\n",
    "    \n",
    "    \n",
    "2. Newton:  $D = (\\nabla^2 f(x^k))^~{-1}~~~~~\\rightarrow d^k = -(\\nabla^2 f(x^k))^{-1}\\nabla f(x^k)~~~~~\\rightarrow~~~~~ x^{k+1} = x^k - \\alpha^k (\\nabla^2 f(x^k))^{-1}\\nabla f(x^k) $\n",
    "\n",
    "    This comes from considering the second order Tylor:\n",
    "    \n",
    "    $\\tilde{f}(x) \\approx f(x^k) + \\nabla f(x^k)(x-x^k)+\\dfrac{1}{2}\\nabla^2f(x^k)(x-x^k)^2$\n",
    "    \n",
    "    This method is generally faster.\n",
    "    \n",
    "    But is computationally expensive because it needs to compute the Gradient, the Hessian, evaluate it and invert it.\n",
    "    \n",
    "    \n",
    "3. Diagonal Scaling: \n",
    "\n",
    "$$D = \\begin{pmatrix}\n",
    "d_1^k & \\cdots  &  & 0  \\\\\n",
    "\\vdots  &d_2^k  &  &   \\\\\n",
    " &  &\\ddots   &  \\vdots\\\\\n",
    "0 &  &  \\cdots & d_n^k \\\\\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "$d_i^k = \\left(\\dfrac{\\partial^2 f(x^k)}{\\partial x_i^2}\\right)^{-1}$\n",
    "\n",
    "\n",
    "So $D$ is a diagonal matrix, which his inverse is trivial and also has only n terms to compute, instead of those nxn of the Newton method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac03c6",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a4797",
   "metadata": {},
   "source": [
    "## How to Choose the step\n",
    "### Already selected $d^k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea656a8",
   "metadata": {},
   "source": [
    "1. Line search: Find the step $\\alpha^k$ that minimize $f$.\n",
    "\n",
    "    $$\\alpha^k~/~f(x^k+\\alpha^k d^k) = min_{\\alpha \\geq 0}f(x^k+\\alpha^k d^k)$$\n",
    "    \n",
    "    <img src='linesearch.png'>\n",
    "\n",
    "    Let's $g(\\alpha) = f(x^k + \\alpha^k d^k)$ with x and d already selected.\n",
    "    \n",
    "    So $g:\\mathbb{R}^+ \\rightarrow\\mathbb{R}$\n",
    "    \n",
    "    The ideal would be derivate and compute the zeros of $g(\\alpha)$\n",
    "    \n",
    "    <img src='linesearch2.png'>\n",
    "    \n",
    "    But sometimes it is not possible, so we have a new optimization problem inside my OP but in only one variable.\n",
    "    \n",
    "    \n",
    "2. Limited Line search: Use a fixed $s>0$ and find \"the best\" $\\alpha^k~\\in~[0, s]$  \n",
    "    \n",
    "    $$\\alpha^k~/~f(x^k+\\alpha^k d^k) = min_{\\alpha \\in [0,s]}f(x^k+\\alpha^k d^k)$$\n",
    "\n",
    "    <img src='limitedLS.png'>\n",
    "    \n",
    "    \n",
    "3. Armijo Rule: Let's fix $0<\\sigma<1$ and $0<\\beta<1$ Usually: $\\sigma \\approx 0.1 $ and $\\beta \\in [1/10, 1/2]$\n",
    "\n",
    "    Star with a step $s$ and keep reducing (m times) by a $\\beta$ $(\\beta^ms)$ factor until $f(x^k)-f(x^k+\\beta^m s d^ k)\\geq -\\sigma\\beta^m \\nabla f(x^k)^Td^k$\n",
    "    \n",
    "    Here the equation is comparig: at the left how much the function decrease, against a factor ($\\sigma$) of how much the function would decrease if it would be linear, at the right.\n",
    "    \n",
    "    <img src='AR2.png'>\n",
    "\n",
    "Here the algorithm took 3 steps to reach the established condition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
